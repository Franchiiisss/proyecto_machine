{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MLP: Clasificaci√≥n Like/Dislike\n",
    "\n",
    "**Objetivo**: Entrenar un MLP para predecir si un usuario dar√° like/dislike a una pel√≠cula.\n",
    "\n",
    "**Autor**: Pf. Rensso Mora Colque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Importaci√≥n de Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, f1_score, precision_recall_curve, auc,\n",
    "    classification_report, confusion_matrix, roc_curve,\n",
    "    accuracy_score, precision_score, recall_score\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('‚úì Librer√≠as importadas correctamente')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Carga de Datos Procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos finales con personas asignadas\n",
    "data = pd.read_csv('data_processed/data_final.csv')\n",
    "user_features = pd.read_csv('data_processed/user_features.csv')\n",
    "movie_features = pd.read_csv('data_processed/movie_features.csv')\n",
    "\n",
    "print('='*60)\n",
    "print('DATOS CARGADOS')\n",
    "print('='*60)\n",
    "print(f'Data: {data.shape}')\n",
    "print(f'User features: {user_features.shape}')\n",
    "print(f'Movie features: {movie_features.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Preparaci√≥n del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir columnas de features\n",
    "genre_columns = ['Action', 'Adventure', 'Animation', 'Children', 'Comedy',\n",
    "                 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir',\n",
    "                 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi',\n",
    "                 'Thriller', 'War', 'Western']\n",
    "\n",
    "feature_cols = (\n",
    "    genre_columns +\n",
    "    ['decade', 'age'] +\n",
    "    ['user_rating_mean', 'user_rating_std', 'user_n_votes', 'user_genre_diversity'] +\n",
    "    [col for col in data.columns if col.startswith('user_pref_')] +\n",
    "    [col for col in data.columns if col.startswith('user_decade_')] +\n",
    "    [col for col in data.columns if col.startswith('user_embed_')] +\n",
    "    [col for col in data.columns if col.startswith('item_embed_')]\n",
    ")\n",
    "\n",
    "# Codificar variables categ√≥ricas\n",
    "le_gender = LabelEncoder()\n",
    "le_occupation = LabelEncoder()\n",
    "\n",
    "data['gender_encoded'] = le_gender.fit_transform(data['gender'])\n",
    "data['occupation_encoded'] = le_occupation.fit_transform(data['occupation'])\n",
    "\n",
    "feature_cols.extend(['gender_encoded', 'occupation_encoded'])\n",
    "\n",
    "# Crear dataset final\n",
    "X = data[feature_cols].copy()\n",
    "y = data['like'].copy()\n",
    "\n",
    "# Imputar valores faltantes\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "print('='*60)\n",
    "print('DATASET FINAL')\n",
    "print('='*60)\n",
    "print(f'Shape de X: {X.shape}')\n",
    "print(f'Shape de y: {y.shape}')\n",
    "print(f'\\nN√∫mero de caracter√≠sticas: {len(feature_cols)}')\n",
    "print(f'\\nDistribuci√≥n de la variable objetivo:')\n",
    "print(y.value_counts())\n",
    "print(f'\\nPorcentaje de likes: {y.mean()*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Partici√≥n Train/Test y Normalizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partici√≥n estratificada\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print('='*60)\n",
    "print('PARTICI√ìN DE DATOS')\n",
    "print('='*60)\n",
    "print(f'Train: {X_train.shape[0]:,} muestras')\n",
    "print(f'Test:  {X_test.shape[0]:,} muestras')\n",
    "print(f'\\nDistribuci√≥n en Train:')\n",
    "print(y_train.value_counts())\n",
    "print(f'\\nDistribuci√≥n en Test:')\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# Normalizaci√≥n\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols)\n",
    "\n",
    "print(f'\\n‚úì Datos normalizados con StandardScaler')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Manejo de Desbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "print('='*60)\n",
    "print('CLASS WEIGHTS')\n",
    "print('='*60)\n",
    "print(f'Clase 0 (Dislike): {class_weights[0]:.4f}')\n",
    "print(f'Clase 1 (Like): {class_weights[1]:.4f}')\n",
    "\n",
    "# Aplicar SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(f'\\n{'='*60}')\n",
    "print('SMOTE - SOBREMUESTREO SINT√âTICO')\n",
    "print('='*60)\n",
    "print(f'Antes de SMOTE:')\n",
    "print(f'  Clase 0: {(y_train == 0).sum():,}')\n",
    "print(f'  Clase 1: {(y_train == 1).sum():,}')\n",
    "print(f'\\nDespu√©s de SMOTE:')\n",
    "print(f'  Clase 0: {(y_train_smote == 0).sum():,}')\n",
    "print(f'  Clase 1: {(y_train_smote == 1).sum():,}')\n",
    "print(f'\\n‚úì Clases balanceadas perfectamente')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Entrenamiento de Modelos MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Baseline (sin manejo de desbalance)\n",
    "print('='*60)\n",
    "print('ENTRENANDO MLP - BASELINE')\n",
    "print('='*60)\n",
    "\n",
    "mlp_baseline = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "mlp_baseline.fit(X_train_scaled, y_train)\n",
    "y_pred_baseline = mlp_baseline.predict(X_test_scaled)\n",
    "y_proba_baseline = mlp_baseline.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(f'‚úì Modelo entrenado en {mlp_baseline.n_iter_} iteraciones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP con Class Weights\n",
    "print('='*60)\n",
    "print('ENTRENANDO MLP - CON CLASS WEIGHTS')\n",
    "print('='*60)\n",
    "\n",
    "mlp_weighted = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "mlp_weighted.fit(X_train_scaled, y_train)\n",
    "y_pred_weighted = mlp_weighted.predict(X_test_scaled)\n",
    "y_proba_weighted = mlp_weighted.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(f'‚úì Modelo entrenado en {mlp_weighted.n_iter_} iteraciones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP con SMOTE\n",
    "print('='*60)\n",
    "print('ENTRENANDO MLP - CON SMOTE')\n",
    "print('='*60)\n",
    "\n",
    "mlp_smote = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64, 32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "mlp_smote.fit(X_train_smote, y_train_smote)\n",
    "y_pred_smote = mlp_smote.predict(X_test_scaled)\n",
    "y_proba_smote = mlp_smote.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(f'‚úì Modelo entrenado en {mlp_smote.n_iter_} iteraciones')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Evaluaci√≥n de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, y_proba, model_name):\n",
    "    \"\"\"Eval√∫a un modelo de clasificaci√≥n\"\"\"\n",
    "    roc_auc = roc_auc_score(y_true, y_proba)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_val = precision_score(y_true, y_pred)\n",
    "    recall_val = recall_score(y_true, y_pred)\n",
    "    \n",
    "    print(f'\\n{'='*60}')\n",
    "    print(f'{model_name}')\n",
    "    print('='*60)\n",
    "    print(f'ROC-AUC:    {roc_auc:.4f}')\n",
    "    print(f'F1-Score:   {f1:.4f}')\n",
    "    print(f'PR-AUC:     {pr_auc:.4f}')\n",
    "    print(f'Accuracy:   {accuracy:.4f}')\n",
    "    print(f'Precision:  {precision_val:.4f}')\n",
    "    print(f'Recall:     {recall_val:.4f}')\n",
    "    \n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'roc_auc': roc_auc,\n",
    "        'f1': f1,\n",
    "        'pr_auc': pr_auc,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision_val,\n",
    "        'recall': recall_val\n",
    "    }\n",
    "\n",
    "# Evaluar los 3 modelos\n",
    "results = []\n",
    "results.append(evaluate_model(y_test, y_pred_baseline, y_proba_baseline, 'MLP Baseline'))\n",
    "results.append(evaluate_model(y_test, y_pred_weighted, y_proba_weighted, 'MLP con Class Weights'))\n",
    "results.append(evaluate_model(y_test, y_pred_smote, y_proba_smote, 'MLP con SMOTE'))\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f'\\n{'='*60}')\n",
    "print('COMPARACI√ìN DE MODELOS')\n",
    "print('='*60)\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Visualizaci√≥n de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de m√©tricas\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "metrics = ['roc_auc', 'f1', 'pr_auc']\n",
    "titles = ['ROC-AUC', 'F1-Score', 'PR-AUC']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "    values = results_df[metric].values\n",
    "    bars = axes[idx].bar(results_df['model'], values, color=colors)\n",
    "    axes[idx].set_title(title, fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Score')\n",
    "    axes[idx].set_ylim([0.5, 1.0])\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for i, (bar, val) in enumerate(zip(bars, values)):\n",
    "        axes[idx].text(bar.get_x() + bar.get_width()/2, val + 0.01,\n",
    "                      f'{val:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curvas ROC y Precision-Recall\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Curva ROC\n",
    "for name, y_proba, color in [\n",
    "    ('Baseline', y_proba_baseline, '#3498db'),\n",
    "    ('Class Weights', y_proba_weighted, '#e74c3c'),\n",
    "    ('SMOTE', y_proba_smote, '#2ecc71')\n",
    "]:\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "    axes[0].plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.4f})',\n",
    "                linewidth=2, color=color)\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random')\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
    "axes[0].set_title('ROC Curves', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Curva Precision-Recall\n",
    "for name, y_proba, color in [\n",
    "    ('Baseline', y_proba_baseline, '#3498db'),\n",
    "    ('Class Weights', y_proba_weighted, '#e74c3c'),\n",
    "    ('SMOTE', y_proba_smote, '#2ecc71')\n",
    "]:\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    axes[1].plot(recall, precision, label=f'{name} (AUC = {pr_auc:.4f})',\n",
    "                linewidth=2, color=color)\n",
    "\n",
    "axes[1].set_xlabel('Recall', fontsize=12)\n",
    "axes[1].set_ylabel('Precision', fontsize=12)\n",
    "axes[1].set_title('Precision-Recall Curves', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc='lower left')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.9 Matriz de Confusi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusi√≥n del mejor modelo (SMOTE)\n",
    "cm = confusion_matrix(y_test, y_pred_smote)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Dislike', 'Like'], yticklabels=['Dislike', 'Like'])\n",
    "axes[0].set_title('Matriz de Confusi√≥n - MLP con SMOTE', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Valor Real')\n",
    "axes[0].set_xlabel('Valor Predicho')\n",
    "\n",
    "# Matriz normalizada\n",
    "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=['Dislike', 'Like'], yticklabels=['Dislike', 'Like'])\n",
    "axes[1].set_title('Matriz de Confusi√≥n Normalizada', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Valor Real')\n",
    "axes[1].set_xlabel('Valor Predicho')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis de errores\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print('='*60)\n",
    "print('AN√ÅLISIS DE ERRORES')\n",
    "print('='*60)\n",
    "print(f'\\nTrue Negatives (TN):  {tn:,}')\n",
    "print(f'False Positives (FP): {fp:,}')\n",
    "print(f'False Negatives (FN): {fn:,}')\n",
    "print(f'True Positives (TP):  {tp:,}')\n",
    "print(f'\\nError Tipo I (FP):  {fp/len(y_test)*100:.2f}%')\n",
    "print(f'Error Tipo II (FN): {fn/len(y_test)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reporte de clasificaci√≥n\n",
    "print('='*60)\n",
    "print('REPORTE DE CLASIFICACI√ìN')\n",
    "print('='*60)\n",
    "print(classification_report(y_test, y_pred_smote,\n",
    "                          target_names=['Dislike', 'Like'],\n",
    "                          digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.10 Validaci√≥n Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validaci√≥n cruzada con el mejor modelo\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print('='*60)\n",
    "print('VALIDACI√ìN CRUZADA - 5 FOLDS')\n",
    "print('='*60)\n",
    "\n",
    "scoring = {\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'f1': 'f1',\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall'\n",
    "}\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    mlp_smote, X_train_smote, y_train_smote,\n",
    "    cv=cv, scoring=scoring, return_train_score=True, n_jobs=-1\n",
    ")\n",
    "\n",
    "for metric in ['roc_auc', 'f1', 'accuracy', 'precision', 'recall']:\n",
    "    test_scores = cv_results[f'test_{metric}']\n",
    "    print(f'\\n{metric.upper()}:')\n",
    "    print(f'  Media: {test_scores.mean():.4f}')\n",
    "    print(f'  Std:   {test_scores.std():.4f}')\n",
    "    print(f'  Min:   {test_scores.min():.4f}')\n",
    "    print(f'  Max:   {test_scores.max():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.11 M√©tricas de Ranking: Hit@K y NDCG@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(relevances, k):\n",
    "    \"\"\"Calcula DCG@K\"\"\"\n",
    "    relevances = np.array(relevances)[:k]\n",
    "    if relevances.size:\n",
    "        return np.sum(relevances / np.log2(np.arange(2, relevances.size + 2)))\n",
    "    return 0.0\n",
    "\n",
    "def ndcg_at_k(relevances, k):\n",
    "    \"\"\"Calcula NDCG@K\"\"\"\n",
    "    dcg = dcg_at_k(relevances, k)\n",
    "    idcg = dcg_at_k(sorted(relevances, reverse=True), k)\n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "    return dcg / idcg\n",
    "\n",
    "def hit_at_k(relevances, k):\n",
    "    \"\"\"Calcula Hit@K\"\"\"\n",
    "    return 1.0 if sum(relevances[:k]) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para ranking\n",
    "test_data = data.loc[X_test.index].copy()\n",
    "test_data['y_true'] = y_test.values\n",
    "test_data['y_pred'] = y_pred_smote\n",
    "test_data['y_proba'] = y_proba_smote\n",
    "\n",
    "# Calcular m√©tricas de ranking por usuario\n",
    "k_values = [5, 10, 20]\n",
    "ranking_metrics = {k: {'hit': [], 'ndcg': []} for k in k_values}\n",
    "\n",
    "for user_id in test_data['user_id'].unique():\n",
    "    user_test_data = test_data[test_data['user_id'] == user_id].copy()\n",
    "    \n",
    "    if len(user_test_data) >= 10:\n",
    "        user_test_data = user_test_data.sort_values('y_proba', ascending=False)\n",
    "        relevances = user_test_data['y_true'].values\n",
    "        \n",
    "        for k in k_values:\n",
    "            ranking_metrics[k]['hit'].append(hit_at_k(relevances, k))\n",
    "            ranking_metrics[k]['ndcg'].append(ndcg_at_k(relevances, k))\n",
    "\n",
    "print('='*60)\n",
    "print('M√âTRICAS DE RANKING')\n",
    "print('='*60)\n",
    "\n",
    "for k in k_values:\n",
    "    hit_k = np.mean(ranking_metrics[k]['hit'])\n",
    "    ndcg_k = np.mean(ranking_metrics[k]['ndcg'])\n",
    "    print(f'\\nK = {k}:')\n",
    "    print(f'  Hit@{k}:  {hit_k:.4f} ({hit_k*100:.2f}%)')\n",
    "    print(f'  NDCG@{k}: {ndcg_k:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de m√©tricas de ranking\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Hit@K\n",
    "hit_values = [np.mean(ranking_metrics[k]['hit']) for k in k_values]\n",
    "axes[0].plot(k_values, hit_values, marker='o', linewidth=2, markersize=10, color='steelblue')\n",
    "axes[0].set_xlabel('K', fontsize=12)\n",
    "axes[0].set_ylabel('Hit@K', fontsize=12)\n",
    "axes[0].set_title('Hit@K - Tasa de aciertos en Top-K', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim([0, 1])\n",
    "for k, hit in zip(k_values, hit_values):\n",
    "    axes[0].text(k, hit + 0.02, f'{hit:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# NDCG@K\n",
    "ndcg_values = [np.mean(ranking_metrics[k]['ndcg']) for k in k_values]\n",
    "axes[1].plot(k_values, ndcg_values, marker='s', linewidth=2, markersize=10, color='coral')\n",
    "axes[1].set_xlabel('K', fontsize=12)\n",
    "axes[1].set_ylabel('NDCG@K', fontsize=12)\n",
    "axes[1].set_title('NDCG@K - Ganancia descontada normalizada', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim([0, 1])\n",
    "for k, ndcg in zip(k_values, ndcg_values):\n",
    "    axes[1].text(k, ndcg + 0.02, f'{ndcg:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.12 Importancia de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular importancia mediante pesos de la primera capa\n",
    "first_layer_weights = np.abs(mlp_smote.coefs_[0])\n",
    "feature_importance = first_layer_weights.mean(axis=1)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print('='*60)\n",
    "print('TOP 20 FEATURES M√ÅS IMPORTANTES')\n",
    "print('='*60)\n",
    "print(importance_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de importancia\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "top_20 = importance_df.head(20)\n",
    "bars = ax.barh(range(len(top_20)), top_20['importance'].values, color='steelblue')\n",
    "ax.set_yticks(range(len(top_20)))\n",
    "ax.set_yticklabels(top_20['feature'].values)\n",
    "ax.set_xlabel('Importancia', fontsize=12)\n",
    "ax.set_title('Top 20 Features M√°s Importantes', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "for i, (bar, val) in enumerate(zip(bars, top_20['importance'].values)):\n",
    "    ax.text(val + 0.001, bar.get_y() + bar.get_height()/2,\n",
    "           f'{val:.4f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.13 Resumen Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print(' '*20 + 'RESUMEN FINAL')\n",
    "print('='*70)\n",
    "\n",
    "print('\\nüìä DATOS:')\n",
    "print(f'  ‚Ä¢ Total de instancias: {len(data):,}')\n",
    "print(f'  ‚Ä¢ Caracter√≠sticas: {len(feature_cols)}')\n",
    "print(f'  ‚Ä¢ Train: {len(X_train):,} | Test: {len(X_test):,}')\n",
    "\n",
    "print('\\nüß† MODELO:')\n",
    "print('  ‚Ä¢ Arquitectura: MLP (128, 64, 32)')\n",
    "print('  ‚Ä¢ Activaci√≥n: ReLU')\n",
    "print('  ‚Ä¢ Optimizador: Adam')\n",
    "print('  ‚Ä¢ Estrategia: SMOTE (mejor rendimiento)')\n",
    "\n",
    "print('\\nüìà RESULTADOS (MLP con SMOTE):')\n",
    "best_results = results_df[results_df['model'].str.contains('SMOTE')].iloc[0]\n",
    "print(f'  ‚Ä¢ ROC-AUC:   {best_results[\"roc_auc\"]:.4f}')\n",
    "print(f'  ‚Ä¢ F1-Score:  {best_results[\"f1\"]:.4f}')\n",
    "print(f'  ‚Ä¢ PR-AUC:    {best_results[\"pr_auc\"]:.4f}')\n",
    "print(f'  ‚Ä¢ Accuracy:  {best_results[\"accuracy\"]:.4f}')\n",
    "print(f'  ‚Ä¢ Precision: {best_results[\"precision\"]:.4f}')\n",
    "print(f'  ‚Ä¢ Recall:    {best_results[\"recall\"]:.4f}')\n",
    "\n",
    "print('\\nüéØ M√âTRICAS DE RANKING:')\n",
    "print(f'  ‚Ä¢ Hit@10:  {np.mean(ranking_metrics[10][\"hit\"])*100:.1f}%')\n",
    "print(f'  ‚Ä¢ NDCG@10: {np.mean(ranking_metrics[10][\"ndcg\"]):.4f}')\n",
    "\n",
    "print('\\n‚úÖ VALIDACI√ìN:')\n",
    "print(f'  ‚Ä¢ 5-Fold Cross-Validation')\n",
    "print(f'  ‚Ä¢ ROC-AUC CV: {cv_results[\"test_roc_auc\"].mean():.4f} ¬± {cv_results[\"test_roc_auc\"].std():.4f}')\n",
    "print(f'  ‚Ä¢ F1 CV: {cv_results[\"test_f1\"].mean():.4f} ¬± {cv_results[\"test_f1\"].std():.4f}')\n",
    "\n",
    "print('\\nüí° CONCLUSIONES:')\n",
    "print('  ‚Ä¢ SMOTE mejora significativamente el rendimiento')\n",
    "print('  ‚Ä¢ El modelo logra buena generalizaci√≥n')\n",
    "print('  ‚Ä¢ Las preferencias de usuario son muy predictivas')\n",
    "print('  ‚Ä¢ Embeddings latentes capturan patrones ocultos')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print(' '*20 + '‚úì PROYECTO COMPLETADO')\n",
    "print('='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.14 Guardar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Guardar modelo y scaler\n",
    "joblib.dump(mlp_smote, 'mlp_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(le_gender, 'le_gender.pkl')\n",
    "joblib.dump(le_occupation, 'le_occupation.pkl')\n",
    "\n",
    "# Guardar resultados\n",
    "results_df.to_csv('data_processed/model_results.csv', index=False)\n",
    "importance_df.to_csv('data_processed/feature_importance.csv', index=False)\n",
    "\n",
    "print('‚úì Modelo y resultados guardados')\n",
    "print('  - mlp_model.pkl')\n",
    "print('  - scaler.pkl')\n",
    "print('  - le_gender.pkl')\n",
    "print('  - le_occupation.pkl')\n",
    "print('  - data_processed/model_results.csv')\n",
    "print('  - data_processed/feature_importance.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
